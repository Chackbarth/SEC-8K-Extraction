import requests
from bs4 import BeautifulSoup
import openai
import csv
import time
import random
import json

# ‚úÖ OpenAI client (insert your key here)
client = openai.OpenAI(api_key="sk-")  # Replace with your actual key

# ‚úÖ SEC headers
HEADERS = {
    "User-Agent": "Your Name Email@Domain.com"
}

# ‚úÖ Fetch ticker/company list from SEC JSON
def get_sp500_companies(num=3):
    url = "https://www.sec.gov/files/company_tickers.json"
    res = requests.get(url, headers=HEADERS)
    data = res.json()
    companies = [v for _, v in sorted(data.items(), key=lambda x: int(x[0]))][:num]
    return [{"ticker": c["ticker"], "cik": str(c["cik_str"]).zfill(10), "name": c["title"]} for c in companies]

# ‚úÖ Atom feed to get most recent 8-K filing link
def get_8k_filing_url(ticker):
    feed_url = f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={ticker}&type=8-K&count=10&output=atom"
    res = requests.get(feed_url, headers=HEADERS)
    soup = BeautifulSoup(res.content, "xml")
    entries = soup.find_all("entry")
    for entry in entries:
        if entry.find("category")["term"] == "8-K":
            index_url = entry.find("link")["href"]
            date = entry.find("updated").text.split("T")[0]
            return index_url, date
    return None, None

# ‚úÖ Parse index page to get the submission .txt file (fallback to .htm)
def get_submission_txt(index_url):
    res = requests.get(index_url, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")
    for row in soup.find_all("tr"):
        cols = row.find_all("td")
        if len(cols) >= 2:
            if "complete submission text file" in cols[1].text.lower():
                link = cols[0].find("a", href=True)
                if link:
                    return "https://www.sec.gov" + link["href"]
    # fallback: try to grab first document if .txt not found
    doc_table = soup.find("table", class_="tableFile", summary="Document Format Files")
    if doc_table:
        rows = doc_table.find_all("tr")[1:]  # Skip header
        for row in rows:
            link = row.find("a", href=True)
            if link and link["href"].endswith(".htm"):
                return "https://www.sec.gov" + link["href"]
    return None

# ‚úÖ OpenAI call
def analyze_with_openai(text, company, ticker, date):
    prompt = f"""
You are an expert SEC analyst. Extract any new product announcements from this SEC 8-K filing.

Respond in this format:
Company Name | Stock Name | Filing Time | New Product | Product Description (max 180 characters)

If no product is found, return:
{company} | {ticker} | {date} | N/A | N/A

Company Name: {company}
Stock Name: {ticker}
Filing Time: {date}

SEC Filing:
{text[:10000]}
"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"{company} | {ticker} | {date} | Error | {str(e)[:100]}"

# ‚úÖ Run the whole process
def run_extraction(num_companies=3):
    companies = get_sp500_companies(num_companies)
    output_rows = []

    for company in companies:
        name, ticker, cik = company["name"], company["ticker"], company["cik"]
        print(f"\nüîç Processing {name} ({ticker})...")
        index_url, filing_date = get_8k_filing_url(ticker)
        if not index_url:
            print("‚ö†Ô∏è No 8-K filing link found.")
            continue

        print(f"üåê Filing index: {index_url}")
        txt_url = get_submission_txt(index_url)
        if not txt_url:
            print("‚ùå Couldn't get submission .txt or fallback .htm")
            continue

        filing_text = requests.get(txt_url, headers=HEADERS).text
        result = analyze_with_openai(filing_text, name, ticker, filing_date)
        print("‚úÖ Extracted:", result)
        output_rows.append(result.split(" | "))
        time.sleep(random.uniform(1, 2))

    if output_rows:
        with open("sec_8k_product_extractions.csv", "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["company_name", "stock_name", "filing_time", "new_product", "product_description"])
            writer.writerows(output_rows)
        print("\n‚úÖ CSV saved as 'sec_8k_product_extractions.csv'")
    else:
        print("\n‚ö†Ô∏è No data extracted. CSV not created.")

# ‚úÖ Run it here
run_extraction(num_companies=150)  # Change as needed
